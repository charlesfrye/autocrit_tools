{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACHTUNG**: This notebook is likely to be slightly broken. It is only partly tested post-`autocrit` and -`autocrit_tools` rewrite. Proceed with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will generate the figures from the arXiV paper \"Numerically Recovering the Critical Points of a Deep Linear Autoencoder\" from the raw critfinder trajectory data.\n",
    "\n",
    "There are a few steps that take an appreciable amount of time on typical consumer hardware:\n",
    "\n",
    "- Downloading the data (2.9 GB, variable time)\n",
    "- Computing the Hessians and indices of critical points for Figures 1, 3, and 4 (20 min)\n",
    "- Re-computing Hessians and indices for changing cutoffs in Figure 2 (10 min)\n",
    "- Timing 100 iterations of each algorithm (15 min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are underestimates for executing this notebook via Binder by approximately a factor of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from IPython.display import SVG\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "import autocrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autocrit_tools.path as path\n",
    "import autocrit_tools.dataframes as dataframes\n",
    "\n",
    "import autocrit_tools.linearpaper.figures as figures\n",
    "import autocrit_tools.linearpaper.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=FutureWarning,\n",
    "    message=\"Using a non-tuple sequence for multidimensional indexing is deprecated;\",\n",
    "    module=\"autograd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_path_columns(df, path_columns, split_str, new_prefix):\n",
    "    for path_column in path_columns:\n",
    "        df[path_column] = df[path_column].apply(clean_path, args=(split_str, new_prefix))\n",
    "        \n",
    "def clean_path(path, split_str, new_prefix):\n",
    "    path_end = path.split(split_str)[-1]\n",
    "    return os.path.join(new_prefix, path_end)\n",
    "\n",
    "def add_index_sets(cp_df):\n",
    "    maps = cp_df.final_theta.apply(\n",
    "        lambda theta: utils.theta_to_map(theta, NETWORK))\n",
    "    index_sets = [utils.map_to_index_set(_map) for _map in maps]\n",
    "\n",
    "    cp_df[\"index_set\"] = index_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures from the paper are included for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_figs_dir = pathlib.Path(\"..\") / \"autocrit_tools\" / \"linearpaper\" / \"im\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the paper is available as a zip at the Dropbox link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_optimization_ID = \"Borirv\"\n",
    "\n",
    "optimizations_dir = os.path.join(\"..\", \"results\", \"optimizations\")\n",
    "\n",
    "target_optimization_dir = os.path.join(optimizations_dir, target_optimization_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_link = \"https://www.dropbox.com/s/dh2ilgnhrv2dxbf/Borirv.zip?dl=1\"\n",
    "\n",
    "if not os.path.exists(target_optimization_dir):\n",
    "    !wget --max-redirect=20 -O Borirv.zip {download_link}\n",
    "    !unzip Borirv.zip -d {optimizations_dir}\n",
    "    !rm Borirv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe of optimizations (combinations of data, network, and optimizer).\n",
    "For the paper, we used only one optimization, so this dataframe as only one row.\n",
    "We pull this row out by its ID and then reconstruct the data and network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = pathlib.Path(\"~\").expanduser() / \"Dropbox\" / \"OptimizationLandscapes\" / \"results_rw1\" \n",
    "\n",
    "data_ID = \"gaussian_16_linspace\"\n",
    "network_ID = \"recreate_icml_20190514\"\n",
    "\n",
    "network_paths = path.ExperimentPaths(data_ID=data_ID, network_ID=network_ID, root=root_dir)\n",
    "\n",
    "target_optimization_ID = \"gd_optimizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df = dataframes.construct_experiments_df(\n",
    "    network_paths.network_dir)\n",
    "\n",
    "target_optimization_row = optimization_df.loc[target_optimization_ID]\n",
    "\n",
    "data, NETWORK, _ = dataframes.reconstruct_from_row(\n",
    "    target_optimization_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_paths = path.ExperimentPaths.from_optimizer_dir(target_optimization_row.optimizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critfinders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trajectories from that optimization (and the network and data) are used to construct critical point finding algorithms, or `critfinders`.\n",
    "\n",
    "These are stored in a dataframe as well, `cf_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critfinders_dir = optimizer_paths.optimizer_dir\n",
    "\n",
    "cf_df = dataframes.construct_experiments_df(\n",
    "    critfinders_dir, experiment_type=\"critfinder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addt'l metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata about hyperparameters, etc., for all of the critfinders is stored inside `finder_kwargs`, so we unpack that into new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_cf_df = pd.DataFrame.from_dict(\n",
    "    dict(cf_df.finder_kwargs_finder)\n",
    "    ).transpose().join(cf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_cf_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Critical Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical critical points are pre-computed and stored in a pickled dataframe.\n",
    "\n",
    "See `etc/make_analytical_cps.py` for details on how these are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.dirname(target_optimization_row.data_path)\n",
    "\n",
    "pickle_path = os.path.join(data_folder, \"analytical_cp_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_cp_df = pd.read_pickle(os.path.join(data_folder, \"analytical_cp_df.pkl\"))\n",
    "\n",
    "analytical_cp_df[\"morse_index\"] = analytical_cp_df.hessian_spectrum.apply(\n",
    "    utils.compute_morse_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Points for Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the figures, we need to select specific critfinders from the dataframe of critfinders, based on their metadata.\n",
    "The function below implements this selection.\n",
    "It then uses the IDs to create dataframes of possible critical points, `cp_df`s, for all selected critfinders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_critfinders(cf_df, method_str, theta_perturbs=None, init_theta=\"uniform_f\", \n",
    "                       min_runs=0, max_runs=1000):\n",
    "    \n",
    "    selector = cf_df.index.str.startswith(method_str) &\\\n",
    "               cf_df.init_theta_experiment.str.endswith(init_theta)\n",
    "    \n",
    "    if theta_perturbs is None:\n",
    "        selector = selector & np.isnan(cf_df.theta_perturb_experiment)\n",
    "    else:\n",
    "        selector = selector & cf_df.theta_perturb_experiment.isin(theta_perturbs)\n",
    "    \n",
    "    cf_ids = expanded_cf_df.index[selector]\n",
    "    \n",
    "    cp_dfs =  utils.make_cp_dfs(cf_ids, cf_df)\n",
    "    \n",
    "    try:\n",
    "        cp_dfs, cf_ids = zip(*[(cp_df, cf_id) for cp_df, cf_id in zip(cp_dfs, cf_ids)\n",
    "                               if (len(cp_df) >= min_runs) & (len(cp_df) <= max_runs)])\n",
    "    except ValueError:\n",
    "        cp_dfs = cf_ids = []\n",
    "    \n",
    "    \n",
    "    return cp_dfs, list(cf_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figures 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first two figures, we are interested in all of the critfinders using a given algorithm that had exactly 15 runs and were initialized with no noise (`theta_perturbs=None`) and uniformly by height (`uniform_f`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MR_cp_dfs, MR_cf_ids = select_critfinders(expanded_cf_df,\n",
    "                                          method_str=\"newtonMR\",\n",
    "                                          min_runs=15, max_runs=15)\n",
    "TR_cp_dfs, TR_cf_ids = select_critfinders(expanded_cf_df,\n",
    "                                          method_str=\"newtonTR\",\n",
    "                                          min_runs=15, max_runs=15)\n",
    "gnm_cp_dfs, gnm_cf_ids = select_critfinders(expanded_cf_df,\n",
    "                                            method_str=\"gnm\",\n",
    "                                            min_runs=15, max_runs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the hyperparameter values for a given list of critfinders and verify their correctness, provide the ids as a list to the metadata DataFrame, `expanded_cf_df`, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_cf_df.loc[MR_cf_ids].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the performance comparison and cutoff figures, we want to combine across runs of a critfinder with the same parameters, but with different initializations, so we merge the cp_dfs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_MR_cp_df = pd.concat(MR_cp_dfs)\n",
    "\n",
    "merged_TR_cp_df = pd.concat(TR_cp_dfs)\n",
    "\n",
    "merged_gnm_cp_df = pd.concat(gnm_cp_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these trajectories do not achieve sufficiently low squared gradient norm, and we want to be able to select these out with `utils.filter_to_candidate_cps`.\n",
    "We also need to add the index information, using `utils.get_hessian_info`.\n",
    "These two functions are combined in `to_candidate_cp` below.\n",
    "\n",
    "For Figure 1, we're interested also in trajectories that failed, so we can optionally return those failed runs with the `return_failures` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_candidate_cp(cp_df, network, return_failures=False):\n",
    "    \n",
    "    if return_failures:\n",
    "        candidate_cp_df, failures_df = utils.filter_to_candidate_cps(\n",
    "            cp_df, network, return_failures=True)\n",
    "        candidate_cp_df[\"morse_index\"] = utils.get_hessian_info(\n",
    "            candidate_cp_df.final_theta, network)[-1]\n",
    "        return candidate_cp_df, failures_df\n",
    "    else:\n",
    "        candidate_cp_df = utils.filter_to_candidate_cps(\n",
    "            cp_df, network, return_failures=False)\n",
    "        candidate_cp_df[\"morse_index\"] = utils.get_hessian_info(\n",
    "            candidate_cp_df.final_theta, network)[-1]\n",
    "        \n",
    "        return candidate_cp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_MR_candidate_cp_df, merged_MR_failures_df = to_candidate_cp(\n",
    "    merged_MR_cp_df, NETWORK, return_failures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_TR_candidate_cp_df, merged_TR_failures_df = to_candidate_cp(\n",
    "    merged_TR_cp_df, NETWORK, return_failures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gnm_candidate_cp_df, merged_gnm_failures_df = to_candidate_cp(\n",
    "    merged_gnm_cp_df, NETWORK, return_failures=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3 - Noise and Bias - Uniform Height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bottom row of Figure 3, we are interested in `newtonMR` runs with additive noise (`theta_perturb != None`).\n",
    "These are selected below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_unif_height_cp_dfs, noisy_unif_height_cf_ids = select_critfinders(\n",
    "    expanded_cf_df, method_str=\"newtonMR\",\n",
    "    theta_perturbs=[-4, -2, -1], init_theta=\"uniform_f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are returned unsorted, so we sort them by `theta_perturbs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dfs_by(key, dfs, cf_ids, cf_df):\n",
    "    l = list(zip(\n",
    "        dfs, cf_df.loc[cf_ids][key].values))\n",
    "    sorted_dfs, sorted_key_vals = zip(*list(sorted(l, key=lambda tup: tup[1])))\n",
    "    return list(sorted_dfs), list(sorted_key_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_unif_height_cp_dfs, noisy_unif_height_theta_perturbs = sort_dfs_by(\n",
    "    \"theta_perturb\", noisy_unif_height_cp_dfs, noisy_unif_height_cf_ids, expanded_cf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_unif_height_candidate_cp_dfs = [to_candidate_cp(cp_df, NETWORK)\n",
    "                                      for cp_df in noisy_unif_height_cp_dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the top-left panel, demonstrating bias, we need critfinders with more runs (50) but without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_height_bias_cp_dfs, unif_height_bias_cf_ids = select_critfinders(\n",
    "    expanded_cf_df, method_str=\"newtonMR\", theta_perturbs=None,\n",
    "    max_runs=50, min_runs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_height_bias_candidate_cp_dfs = [to_candidate_cp(cp_df, NETWORK)\n",
    "                                     for cp_df in unif_height_bias_cp_dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bottom row, we also compare to a run without noise, in order to show that low noise values don't change the set of critical points that is recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_height_noiseless_cp_dfs, unif_height_noiseless_cf_ids = select_critfinders(\n",
    "    expanded_cf_df, method_str=\"newtonMR\", theta_perturbs=None,\n",
    "    max_runs=100, min_runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_height_noiseless_candidate_cp_dfs = [to_candidate_cp(cp_df, NETWORK)\n",
    "                                          for cp_df in unif_height_noiseless_cp_dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 4 - Noise and Bias - Uniform Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4 is the same as Figure 3, but with `init_theta == uniform`, referred to in the paper as \"Uniform Iteration\", rather than `uniform_f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_unif_iteration_cp_dfs, noisy_unif_iteration_cf_ids = select_critfinders(\n",
    "    expanded_cf_df, method_str=\"newtonMR\",\n",
    "    theta_perturbs=[-4, -2, -1], init_theta=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_unif_iteration_cp_dfs, noisy_unif_iteration_theta_perturbs = sort_dfs_by(\n",
    "    \"theta_perturb\", noisy_unif_iteration_cp_dfs,\n",
    "    noisy_unif_iteration_cf_ids, expanded_cf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_unif_iteration_candidate_cp_dfs = [to_candidate_cp(cp_df, NETWORK)\n",
    "                                         for cp_df in noisy_unif_iteration_cp_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_iteration_bias_cp_dfs, unif_iteration_bias_cf_ids = select_critfinders(\n",
    "    expanded_cf_df, method_str=\"newtonMR\", theta_perturbs=None, init_theta=\"uniform\",\n",
    "    max_runs=50, min_runs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_iteration_bias_candidate_cp_dfs = [to_candidate_cp(cp_df, NETWORK)\n",
    "                                        for cp_df in unif_iteration_bias_cp_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_iteration_noiseless_cp_dfs, unif_iteration_noiseless_cf_ids = select_critfinders(\n",
    "    expanded_cf_df, method_str=\"newtonMR\", theta_perturbs=None, init_theta=\"uniform\",\n",
    "    max_runs=100, min_runs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_iteration_noiseless_candidate_cp_dfs = [to_candidate_cp(cp_df, NETWORK)\n",
    "                                             for cp_df in unif_iteration_noiseless_cp_dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1 - Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_cp_dfs = [merged_MR_candidate_cp_df,\n",
    "                merged_TR_candidate_cp_df,\n",
    "                merged_gnm_candidate_cp_df,\n",
    "                    ]\n",
    "\n",
    "failed_cp_dfs = [merged_MR_failures_df, merged_TR_failures_df, merged_gnm_failures_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titles = [\"Newton-MR\", \"Newton-TR\", \"GNM\"]\n",
    "f, axs = figures.make_performance_comparison_figure(\n",
    "    candidate_cp_dfs, analytical_cp_df, titles=titles,\n",
    "    failed_cp_dfs=failed_cp_dfs, base_figure_scale=4);\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(str(saved_figs_dir / \"fig-algorithm-performance.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_iter_count(cp_df):\n",
    "    cp_df[\"num_iters\"] = cp_df.thetas.apply(lambda thetas: len(thetas)-1)\n",
    "    return cp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_MR_candidate_cp_df = add_iter_count(merged_MR_candidate_cp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_TR_candidate_cp_df = add_iter_count(merged_TR_candidate_cp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_MR_candidate_cp_df.num_iters.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_TR_candidate_cp_df.num_iters.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of iterations is strongly non-Gaussian, so we use the Mann-Whitney $U$ test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.mannwhitneyu(merged_MR_candidate_cp_df.num_iters,\n",
    "                         merged_TR_candidate_cp_df.num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failed Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient norm minimization fails at a much higher rate than the other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_gnm_failures_df)/len(merged_gnm_cp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wall Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that results here won't precisely match those in the paper, since the compute resources will likely be different.\n",
    "It should still be the case that GNM is faster per iteration than Newton-MR, which is faster than Newton-TR.\n",
    "It should also still be the case that GNM is less than 100 times faster per iteration than Newton-MR (for us, approximately only 4 times faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_theta = merged_MR_candidate_cp_df.iloc[0].thetas[0]\n",
    "\n",
    "example_MR_run = expanded_cf_df.loc[MR_cf_ids].iloc[0]\n",
    "MR_kwargs = example_MR_run.finder_kwargs\n",
    "\n",
    "MR_kwargs[\"log_kwargs\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtonMR_experiment = critfinder.FastNewtonMR(NETWORK.loss, **MR_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "theta = newtonMR_experiment.run(init_theta, iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_theta = merged_TR_candidate_cp_df.iloc[0].thetas[0]\n",
    "\n",
    "example_TR_run = expanded_cf_df.loc[TR_cf_ids].iloc[0]\n",
    "\n",
    "TR_kwargs = example_TR_run.finder_kwargs\n",
    "\n",
    "TR_kwargs[\"log_kwargs\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtonTR_experiment = critfinder.finders.newtons.FastNewtonTR(NETWORK.loss, **TR_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "theta = newtonTR_experiment.run(init_theta, iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_theta = merged_gnm_candidate_cp_df.iloc[0].thetas[0]\n",
    "\n",
    "gnm_kwargs = expanded_cf_df.loc[gnm_cf_ids].iloc[0].finder_kwargs\n",
    "\n",
    "gnm_kwargs[\"log_kwargs\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnm_experiment = critfinder.finders.gradnormmin.GradientNormMinimizer(NETWORK.loss,\n",
    "                                                                      **gnm_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "theta = gnm_experiment.run(init_theta, num_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2 - Cutoffs Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare cutoffs by pretending that we stopped iteration as soon as the squared gradient norm reached the cutoff value.\n",
    "\n",
    "`compute_cutoff_cp_dfs` below implements this on a raw `cp_df` (one that has not been filtered to candidate cps!) by calling `filter_to_candidate_cps` with the `cut_early` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cutoff_cp_dfs(base_cp_df, cutoffs, network=NETWORK):\n",
    "    \n",
    "    cutoff_cp_dfs = []\n",
    "    cutoff_morse_index_lists = []\n",
    "    \n",
    "    for cutoff in cutoffs:\n",
    "        \n",
    "        cutoff_cp_df = utils.filter_to_candidate_cps(base_cp_df,\n",
    "                                                     network, cutoff,\n",
    "                                                     cut_early=True)\n",
    "        \n",
    "        _, _, morse_indices = utils.get_hessian_info(cutoff_cp_df.candidate_theta,\n",
    "                                               network)\n",
    "        cutoff_cp_df[\"morse_index\"] = morse_indices\n",
    "        cutoff_cp_dfs.append(cutoff_cp_df)\n",
    "        \n",
    "    return cutoff_cp_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cell below takes a few minutes to run on typical hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cutoffs = [1e-10, 1e-8, 1e-6, np.inf]\n",
    "\n",
    "cutoff_cp_dfs = compute_cutoff_cp_dfs(merged_MR_cp_df, cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = figures.make_cutoff_comparison_figure(\n",
    "    cutoff_cp_dfs, cutoffs, analytical_cp_df, base_figure_scale=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(str(saved_figs_dir / \"fig-cutoffs.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3 - Sampling Bias and Noise - Uniform Height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figures 3 and 4 are the same visualization applied to different data.\n",
    "In Figure 3, we apply it to critfinders whose initial points were sampled uniformly with respect to loss value (\"Uniform Height\", in the paper).\n",
    "\n",
    "In the top left, we have the distribution of eigenvector IDs onto which recovered critical points performed projection.\n",
    "These come from the `bias_cp_dfs`, since they show the bias towards low eigenvector IDs.\n",
    "\n",
    "The effect of varying levels of additive noise on this bias is examined in this figure.\n",
    "The results come from the `noisy_unif_{height,iter}_candidate_cp_dfs`.\n",
    "\n",
    "The entropy of the eigenvector distributions is computed and compared in the top right panel.\n",
    "This will be applied to all `cp_df`s provided as the `entropy_cp_dfs` argument to `make_bias_noise_figure`.\n",
    "\n",
    "The `noisy` critical points are compared to the analytical critical points from `analytical_cp_df` and to the critical points found by their noiseless version, provided as the `noiseless_cp_df` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_height_bias_cp_df_labels = [\"Trajectory \" + str(ii+1)\n",
    "                                 for ii in range(len(unif_height_bias_cp_dfs))]\n",
    "[add_index_sets(cp_df) for cp_df in unif_height_bias_candidate_cp_dfs]\n",
    "\n",
    "unif_height_noise_levels = [10 ** theta_perturb\n",
    "                            for theta_perturb in noisy_unif_height_theta_perturbs]\n",
    "[add_index_sets(cp_df) for cp_df in noisy_unif_height_candidate_cp_dfs]\n",
    "\n",
    "unif_height_entropy_cp_dfs = [unif_height_noiseless_candidate_cp_dfs[0]]\\\n",
    "                             + list(noisy_unif_height_candidate_cp_dfs)\n",
    "[add_index_sets(cp_df) for cp_df in unif_height_entropy_cp_dfs]\n",
    "\n",
    "unif_height_entropy_cp_df_labels = [\"Uniform\\nHeight\"] +\\\n",
    "                                   [r\"\"\"$\\sigma={0}$\"\"\".format(noise_level)\n",
    "                                    for noise_level in unif_height_noise_levels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, _, _, _ = figures.make_bias_noise_figure(\n",
    "                       unif_height_bias_candidate_cp_dfs,\n",
    "                       unif_height_entropy_cp_dfs,\n",
    "                       noisy_unif_height_candidate_cp_dfs,\n",
    "                       unif_height_noise_levels,\n",
    "                       unif_height_noiseless_candidate_cp_dfs[0],\n",
    "                       analytical_cp_df,\n",
    "                       bias_cp_df_labels=unif_height_bias_cp_df_labels,\n",
    "                       entropy_cp_df_labels=unif_height_entropy_cp_df_labels,\n",
    "                       base_figure_scale=5);\n",
    "\n",
    "for ax in f.axes:\n",
    "    if not ax.has_data():\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(str(saved_figs_dir / \"fig-bias-unif-height.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy is computed from the results of a run.\n",
    "Since the runs are independent and identically distributed, we can bootstrap resample them to get a sense of the variability in the entropies and perform statistical testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_entropies(cp_df, num_bootstraps=100):\n",
    "    return [utils.compute_entropy(cp_df.sample(len(cp_df), replace=True))\n",
    "            for _ in range(num_bootstraps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = [utils.compute_entropy(cp_df) for cp_df in unif_height_entropy_cp_dfs]\n",
    "\n",
    "entropy_bootstraps = [bootstrap_entropies(cp_df)\n",
    "                      for cp_df in unif_height_entropy_cp_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.hist(entropy_bootstrap, label=ii, histtype=\"step\", linewidth=2)\n",
    " for ii, entropy_bootstrap\n",
    " in enumerate(entropy_bootstraps)];\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(entropy_bootstraps[0]), np.mean(entropy_bootstraps[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(entropy_bootstraps[0], entropy_bootstraps[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4 - Sampling Bias and Noise - Uniform Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Figure 3 description.\n",
    "\n",
    "Figure 4 uses critfinders whose initial points are sampled randomly from the optimization trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_iteration_bias_cp_df_labels = [\"Trajectory \" + str(ii+1)\n",
    "                                    for ii in range(len(unif_iteration_bias_cp_dfs))]\n",
    "[add_index_sets(bias_cp_df) for bias_cp_df in unif_iteration_bias_candidate_cp_dfs]\n",
    "\n",
    "unif_iteration_noise_levels = [10 ** theta_perturb\n",
    "                            for theta_perturb in noisy_unif_iteration_theta_perturbs]\n",
    "[add_index_sets(cp_df) for cp_df in noisy_unif_iteration_candidate_cp_dfs]\n",
    "\n",
    "unif_iteration_entropy_cp_dfs = [unif_height_noiseless_candidate_cp_dfs[0],\n",
    "                                 noisy_unif_height_candidate_cp_dfs[1],\n",
    "                                 unif_iteration_noiseless_candidate_cp_dfs[0],\n",
    "                                 noisy_unif_iteration_candidate_cp_dfs[1]]\n",
    "\n",
    "[add_index_sets(entropy_cp_df) for entropy_cp_df in unif_iteration_entropy_cp_dfs]\n",
    "unif_iteration_entropy_cp_df_labels = [\"Uniform\\nHeight\",\n",
    "                                       r\"\"\"Unif Height,\n",
    " $\\sigma=${0}\"\"\".format(1e-2),\n",
    "                                       \"Uniform\\nIteration\",\n",
    "                                       r\"\"\"Unif Iteration,\n",
    " $\\sigma=${0}\"\"\".format(1e-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, _, _, _ = figures.make_bias_noise_figure(\n",
    "                       unif_iteration_bias_candidate_cp_dfs,\n",
    "                       unif_iteration_entropy_cp_dfs,\n",
    "                       noisy_unif_iteration_candidate_cp_dfs,\n",
    "                       unif_iteration_noise_levels,\n",
    "                       unif_iteration_noiseless_candidate_cp_dfs[0],\n",
    "                       analytical_cp_df,\n",
    "                       bias_cp_df_labels=unif_iteration_bias_cp_df_labels,\n",
    "                       entropy_cp_df_labels=unif_iteration_entropy_cp_df_labels,\n",
    "                       base_figure_scale=5);\n",
    "\n",
    "for ax in f.axes:\n",
    "    if not ax.has_data():\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(str(saved_figs_dir / \"fig-bias-unif-iteration.svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = [utils.compute_entropy(cp_df) for cp_df in unif_iteration_entropy_cp_dfs]\n",
    "\n",
    "entropy_bootstraps = [bootstrap_entropies(cp_df)\n",
    "                      for cp_df in unif_iteration_entropy_cp_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.hist(entropy_bootstrap, label=ii, histtype=\"step\", linewidth=2)\n",
    " for ii, entropy_bootstrap\n",
    " in enumerate(entropy_bootstraps)];\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(entropy_bootstraps[0]), np.mean(entropy_bootstraps[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(entropy_bootstraps[0], entropy_bootstraps[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(entropy_bootstraps[0]), np.mean(entropy_bootstraps[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(entropy_bootstraps[0], entropy_bootstraps[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
